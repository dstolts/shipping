---
layout: post
title:  "How Azure, Docker, Nano and DevOps Enabled a Global Company to Streamline Application Deployment"
author: "Dan Stolts"
author-link: "#"
#author-image: "(/images/2016-01-12-shipping/authors/DanStolts.png"
date:   2016-12-05
categories: DevOps
color: "blue"
#image: "images/2016-01-12-shipping/imagename.png" #should be ~350px tall
excerpt: In a global enterprise there are often many departments and people involved with the application pipeline. Going from idea to production and then into the feedback loop and redeploying could encompass dozens of departments and 50 or more people. In this 2 day hackfest Docker and Microsoft worked together to show one such enterprise how they could go from many months for deployment and updates down to many a day. Please read about this solution to understand this journey and the solutions implemented.  
language: English
verticals: [Business to Business]
---

In a global enterprise there are often many departments and people involved with the application pipeline. Going from idea to production and then into the feedback loop and redeploying could encompass dozens of departments and 50 or more people. In this 2 day hackfest Docker and Microsoft worked together to show one such enterprise how they could go from many months for deployment and updates down to many deployments a day. Please read about this solution to understand this journey and the solutions implemented. This hackfest had a duel purpose.  The customer was interested in Docker, Windows Server 2016 Nano and Azure.  They also wanted to learn how they could leverage DevOps practices to greatly simplify the application pipeline. Almost all of their applications and services were on-premises so understanding how a machine could be moved from on-premises to the cloud was very important. In this hackfest we made use of the following practices, services and technologies:

- Infrastructure as Code
- Automated Deployment (scripts)
- Linux VMs (with Docker)
- Nano VMs with Docker
- Azure Blob Storage
- Azure VMs
- Docker Desktop on Windows 10
- Windows Server 2016 Nano Server
- Windows Server 2016 Hyper-V

The project took place over the course of 4 weeks. We had a couple meetings leading up to the hackfest and a few more meetings following up from the hackfest to complete the work we started at the hackfest. 

- Global Enterprise
  - B.B. - Infrastructure Architect - service fabric, containers
  - A.S. - Application Architect -  setting up patterns and practices CI/CD Pipeline; drive how teams work in enterprise
  - S.G. - Application Architect -  Docker, Azure Service Fabric 
  - D.S. - Principle Infrastructure Architect - DevOps Evangelist - OSS expert
  - D.J. - Infrastructure Architect - Windows Infrastructure
- Microsoft
  - Dan Stolts – Senior Technical Evangelist, Microsoft, [@itproguru](https://twitter.com/itproguru) - Trainer, DevOps Lead, Primary Dev
  - Ian Philpot – Senior Technical Evangelist, Microsoft, [@tripdubroot](https://twitter.com/tripdubroot) - Trainer
  - Adina Shanholtz – Technical Evangelist, Microsoft - Cross Platform Dev, Assist as needed
  - Heather Shapiro – Technical Evangelist, Microsoft, [@microheather](https://twitter.com/microheather) - Monitoring Hackfest
  - Rachel White - Monitoring Hackfest
  - Andrew Reitano –  Monitoring hackfest
  - Many others assisted in coordination and remote support for hackfest

All source code can be found on [github](http:/github.com/dstolts/shipping)

![Shipping on Github](/images/2016-01-12-shipping/shipping-siteimage.png)


>>>>>

## Customer profile ##
Global Shipping Company that ships all across the US and around the world. Logistics and distribution; transportation and freight (air, sea, ground, rail); freight forwarding to 195 countries; international trade management and customs brokerage. They are a leading provider of less-than-truckload and truckload services coast-to-coast.

## Customer testimony ##
- The session was very insightful on docker container on windows. Would want to have next session on CICD pipeline with docker container. Very good learning session. Thanks a ton to “The ITProGuru”.
- Having the event at Microsoft and disconnected from or day jobs; allowed us to truly focus on the task at hand. In a very busy world this is invaluable.
- Dan, You did a great job facilitating the event and kept us focused, thanks for a job well done.
- The exercise to map existing development and deployment processes really starts to bring to light the areas in which massive productivity gains can be made.
- The participation of resources from different areas really helps share what the other side does and the pains they feel.
- Touching technology as part of the learning processes really accelerates the consumption of new information.
-	Automate, automate, automate… or else you fail.

## Problem statement ##
Need to bring quality products to market faster.  Need to shorten process to improve value stream. Need to learn how to apply DevOps practices for development pipeline. Need to change from a monolythic approach to application development to one that can leverage deployment through Docker. There are too many challenges to overcome in a short period of time to get a production application through a new pipeline. Even worse, to get a simple hotfix out can take 6 weeks or more. We ultimately would prefer to have TFS, Windows Containers and Docker with service fabric orchestration of the CI/CD pipeline.  A large majority of our services are currently running on Windows Server so we would like to focus on Docker running on Nano.  We also have some linux based apps so we would like to understand the similarities and differences of the processes based on platform. We want to make sure we can easily move between on-premises and the cloud if needed. We must be able to respond faster to our customer needs and complaints.  

## Problem Description ##
How to tackle DevOps in a big business with hundreds of apps and even more processes is a huge challenge.  A process is needed that will allow delivery of features and capabilities to customers in a very short period of time. Need a process that will allow us to leverage continuous integration, with continuous development and delivery.  Determine the best way to apply DevOps in complex App Cycle. To solve this large of a problem, it is hard to even understand where to start. We would like to start with deploying a single app to Azure on Docker. We need to fully vet out the pipeline process before production code can be sent through it so we will work with a simple sample app instead of production application.  

## Project Scope ##
Scope should be limited to what can be reasonably be accomplished in a two day hackfest.  We will have at least 2 Developers, 2 Operations, 1 Architect and 1 Pipeline Manager. Training will also need to be done as part of the hackfest so the actual hack time will be limited to approximately 1.5 days. In this period we will stand up a Nano server on Hyper-v, connect to it, install docker, deploy an application in a docker container.  We will then uplade that image to Azure and create a new VM running docker out of that VHD. On the Linux side, we just want to be able to create a Linux machine on Azure with Docker installed. Our language of choice is PowerShell which can be very easily automated.  Beyond scope will be fully automating this process. Instead we will create the scripts to do the work so they can be levereaged in the CI/CD process.  If there is time, we would also like to look at automating the CI/CD process. 

## Solution, steps, and delivery ##

### Creating the Value Stream Map ###

The value stream mapping portion of the project helped recognize areas of waste and how much could be saved, in terms of time, if the process was streamlined.  

*The exercise to map existing development and deployment processes really starts to bring to light the areas in which massive productivity gains can be made.*

![Shipping on Github](/images/CurrentPipeline.jpg)

By stepping through the current pipeline it was easy to see it took months just in the planning phase of the project. The process was designed in such a way to try to make it easy to execute by both operations and development. Sr. Architects, Operations, and eventually even development would be brought into the design process.  The output of this process was a design specifications document that would be handed off to both operations and development to execute against. This design process took 9-24 weeks to create. During the design phase, all relavent departments were brought in to make sure everyone was on the same page. However, the actual result was there were still many weeks of lag and wasted time in the process due to the plan/design not being complete or effective.  Additionally, there were departments that were not involved that would eventually hinder the process before the app could be deployed. One example of this is AD. AD was not generally engaged until the infrastructure was all built, the code was all done and ready to send to QA.  Before sending to QA, the dev team needed to test with real credentials instead of the dummy "dev" credentials that were used to build out the solution. The developers built their own infrastructure using a self-service portal from Operations. They used all local credentials for all coding and testing. The reality is, whole departments and processes are missed all the time. Another DevOps practice of constant feedback can help eliminate this and many other common issues. Constant feedback is not just for the customer but for everyone in the pipeline.

We actually had to do a bunch of back and forth to follow the entire process and found some gaps, some were outside the perview of anyone involved with the VSM.  To make this easier to see, a Visio version of the current state was created. In this it is easy to see:
- QA is a black hole.  Nobody knows what QA actually does or how long it takes to get into production
- There is no communication or handoff between Dev and Ops. These pieces do not come together until QA
- Feedback loop is not functional.  It only provides very basic feedback from the customer to the support group.  Then the entire process starts over to apply a fix or make a change.
- Additional challenges and gaps were found but it became clear, it would be easier to start off with creating a new streamline process than to try to fill the gaps one at a time. 

![Shipping on Github](/images/VSMCurrent.png)

With the limited time, it was decided to zero in on building the infrastructure as code, teach both dev and ops how they could work together with a more streamlined process.  As a result an internal project was started to completely overhaul the pipeline into a green and highly optimized environment leveraging Docker. The Linx platform of this project has already been approved and it is expected the Windows platform will also be approved.  


### Technical Details ###
The hack started with training on DevOps, Docker, Nano and Azure. This was a number of slides then some whiteboarding to dive into the container process and highlight the similarities between Linux and Windows. Since both Windows and Linux use the same docker commands and the same exact syntax, leaning one would make learning the other super simple. The only differences is in the startup code and bits for these configurations. Expectations we set for what we could likely get done in the time of the hack.  The most important thing was to prove out a hybrid scenario. Target was set to create a Nano VM in Azure. Copy a container to Azure. Make a change then copy it back to laptop.  To this end, both and on-prem and azure infrastructure needed to be configured.  We would also need a private docker registry.  For now, we will use a user private registry. Also in the future and if time is available, we would leverage the code we created to automate.  There was not any expectation that CI/CD could be achived, but it was left on the to do list which will be done in a future hack or future enhancements to the process we created.  

![Shipping on Github](/images/HackSteps.jpg)

We split into two teams.  One team worked on building nano in azure while the other worked on building on Hyper-V.  Due to corporate laptop policies, Docker could not be installed locally.  One use had a personal laptop with them so that was used for the local part of the hybrid solution. Dan had a laptop with Hyper-V enabled so he worked on creating the process on Hyper-V which is where the eventual on-prem solution would land. Many scripts were created to for the solution.






The majority of your win artifacts will be included in this section, including (but not limited to) the following: Pictures, drawings, value stream mappings, architectural diagrams and demo videos.


This section should include the following details:

- Technical details of how this was implemented.

- What was worked on and what problem it helped solve.

- DevOps practice area improved.

- Pointers to references or documentation.
 
- Learnings from the Microsoft team and the customer team.



## General lessons ##
Some key learnings to consider from this process:
 
## Conclusion ##

## Resources ##
  - Source Code on [github](http:/github.com/dstolts/shipping) 