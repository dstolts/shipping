---
layout: post
title:  "How Azure, Docker and DevOps Hackfest Enabled a Global Company to Streamline Application Deployment"
author: "Dan Stolts"
author-link: "#"
#author-image: "(/images/2016-01-12-shipping/authors/DanStolts.png"
date:   2016-12-05
categories: DevOps
color: "blue"
#image: "images/2016-01-12-shipping/imagename.png" #should be ~350px tall
excerpt: In a global enterprise there are often many departments and people involved with the application pipeline. Going from idea to production and then into the feedback loop and redeploying could encompass dozens of departments and 50 or more people. In this 2 day hackfest Docker and Microsoft worked together to show one such enterprise how they could go from many months for deployment and updates down to many a day. Please read about this solution to understand this journey and the solutions implemented.  
language: English
verticals: [Business to Business]
---

In a global enterprise there are often many departments and people involved with the application pipeline. Going from idea to production and then into the feedback loop and redeploying could encompass dozens of departments and 50 or more people. In this 2 day hackfest Docker and Microsoft worked together to show one such enterprise how they could go from many months for deployment and updates down to many deployments a day. Please read about this solution to understand this journey and the solutions implemented. This hackfest had a duel purpose.  The customer was interested in Docker, Windows Server 2016 Nano and Azure.  They also wanted to learn how they could leverage DevOps practices to greatly simplify the application pipeline. Customer already started a OSS DevOps pipeline but wanted to focus on Windows and pick up any additional ideas that may help with their OSS ongoing project. In this hackfest we made use of the following practices and services:

- Infrastructure as Code
- Automated Deployment (scripts)
- Linux VMs (with Docker)
- Nano VMs with Docker
- Azure Container Service 
- Azure Blob Storage
- Docker Desktop on Windows 10

The project took place over the course of 4 weeks. We had a couple meetings leading up to the hackfest and a few more meetings following up from the hackfest to complete the work we started at the hackfest. 

- Global Enterprise
  - B.B. - Infrastructure Architect - service fabric, containers
  - A.S. - Application Architect -  setting up patterns and practices CI/CD Pipeline; drive how teams work in enterprise
  - S.G. - Application Architect -  Docker, Azure Service Fabric 
  - D.S. - Principle Infrastructure Architect - DevOps Evangelist - OSS expert
  - D.J. - Infrastructure Architect - Windows Infrastructure
- Microsoft
  - Dan Stolts – Senior Technical Evangelist, Microsoft, [@itproguru](https://twitter.com/itproguru) - Trainer and DevOps Lead
  - Ian Philpot – Senior Technical Evangelist, Microsoft, [@tripdubroot](https://twitter.com/tripdubroot) - Trainer
  - Adina Shanholtz – Technical Evangelist, Microsoft - Cross Platform Dev, Assist as needed
  - Heather Shapiro – Technical Evangelist, Microsoft, [@microheather](https://twitter.com/microheather) - Monitoring Hackfest
  - Rachel White - Web developer, cognitive services, web apps - Monitoring Hackfest
  - Andrew Reitano – EE, IoT - Monitoring hackfest
  - Many others assisted in coordination and remote support for hackfest

All source code can be found on [github](http:/github.com/dstolts/shipping)

![Shipping on Github](/images/2016-01-12-shipping/shipping-siteimage.png)


>>>>>

## Customer profile ##
This Major US University is a world-class university that is known for their leading-edge stance on the utilization of technology. The University is regularly regarded as a top 5 overall university worldwide. The Institute is an independent, coeducational, privately endowed university, organized into five Schools (architecture and planning; engineering; humanities, arts, and social sciences; management; and science). It has some 1,000 faculty members, more than 11,000 undergraduate and graduate students and more than 130,000 living alumni. They would, for now, prefer to remain anonymous so this document will refer to this University as simply "The University". The University provides its students with a platform to manage their infrastructure, submit homework, collaborate in teams, collaborate with professors. It gives students a single command to run to log in to their automatically generated hardware infrastructure, provides students the capability to stand up the additional infrastructure so they can thoroughly evaluate the performance of the software they create on different classes of machines and even clusters of servers. A large (6 or more) Teacher Assistants (TA) staff helps students as needed so they are also automatically granted access rights to the machines the students use. These TA's manage many aspects of the class including grading all homework, making sure the students are fully prepared to start class on day 1 with no technical surprises. They also manage the backend infrastructure that is used for submitting homework and exams. 

## Customer testimony ##

## Problem statement ##

### Creating the Value Stream Map

The value stream mapping portion of the project helped The University see the big picture and understand where automation, proper processes, and DevOps practices can improve processes, expand capabilities, decrease setup time, simplify standing up a teaching environment and the value of monitoring the students and the process.

This process identified that the classroom environment is very different from a traditional enterprise or small company environment.  For this project, we were working with grad students as the workers (not professional developers). The programs were revised but they were not deployed in a typical deployment fashion. We discussed some ideas of how we could integrate full CI/CD but due to the very tight timeline we had for this project (before class starts) we had to limit the scope to automating delivery through scripts that can later be used to expand into CI/CD. To account for these changes we focused most of our efforts on designing Infrastructure as Code that would be the "Application" that they are deploying.   

Value Stream Maps (VSM) are a great vehicle to understand an existing workflow and to decide what areas to focus on for improvement. The diagram below is an example classroom development environment. The student VM is a "jump box" used for development. The student submits a Job message to the Job Cluster. The job cluster runs various tests and the student's grade gets calculated based on run time.  The report/grade is put in a private share for that student. You will notice the times to perform these various processes are extensive.  Weeks to build out the infrastructure and docs for each class.  Then the hard part is for students.  Students often needed to start setting up their environment before class started because for many it would take many days.  Some did not even get it done by Day 5 which was the day that the first homework assignment was due.  This caused the students to be late in submitting their homework.  For students that did not engage until day one of class, or enrolled late, they were in bad shape.  Most if not all of these students needed 1:1 TA support to get their systems and environment working correctly.  We expected to eliminate most of these delays which were caused by manual processes and confusion.  We expected development to take 3-4 weeks and student onboarding to take a day.  The resulting code took more than 6 weeks to develop on the calendar; 4 weeks engineer time; and less than a day to onboard the students.  Going over budget on the time calendar was mostly due to scope changes going through the project. Example, changing ARM Templates to Azure CLI.

The staff scheduled a "lab" to onboard students on the Saturday before class for students to get their environment set up with Microsoft and TA's in the room to assist as needed.  We had a couple changes that we had to make on the fly, redeployed the code mods a few times but get the entire class of over 100 students done in one day.  Most students were done in less than an hour.  The lab was scheduled in 4 different rooms and at 3 different time slots of 4 hours so we could accommodate all the students.  Approximately 90% were successful on their own with no TA interaction with just a URL on the board for where to go for the onboarding instructions.  The other 10% were fixed through tweaks (check for scenarios and accommodate) to the onboarding script, redeploying it to production and having those students try again [pseudo CI/CD]. Other than these few tweaks, Microsoft involvement was not needed. Clearly, this could all be done by staff moving forward. All students that showed up for the setup lab had their entire lab set up, logged into VMs, homework submission and running a test script on their VM before they left (1-2 hours each).  This far surpassed our goal of onboarding students with one day of student effort. It also almost completely eliminated the stress of helping students onboard. Most importantly the process was in place to repeat for future classes with little or no effort. 

During VSM we determined shared storage was going to be a requirement.  Students needed a way to send private work to the staff where other students could not see or otherwise gain access.  There was also a need to have students share files with other students.  This would allow for team collaboration.  This structure would need to be one of the first things we set up in order to accommodate connection from all other elements of the infrastructure. By having a separate script to define storage we could very easily isolate that component and build additional drives that could easily be mounted by students if it becomes needed during class. 

The value stream map helped identify opportunities for automating current manual processes.  It allowed the team to visualize scenarios that could improve the deliverables, streamline existing process and even eliminate processes that could be fully automated.  Examples of this include: authenticating using .EDU credentials, eliminating the need for students to manually sign-up for a cloud account and monitoring and tracking usage by the student. It showed how full CI/CD, automated testing and even automated destruction could greatly decrease manual processes while eliminating errors. Prior to the value stream map, these capabilities were not even in the thought process. Additionally, the value stream map provided additional areas of improvement that can be incorporated as changes made for future classes.

![Creating the value stream map](\images\2016-12-05-classroom\Azure_Classroom_VSM.jpg)

There were many manual processes that were identified for future work by the customer.  There were also a couple areas where heroics were identified.  In these cases, it was determined that others needed to be trained to take on some or all of those tasks at least for backup.  The heroics identified mostly revolved around one person that will only be in the program for a couple more years so it is important that this training starts now.  Documentation would also include ramping future TA's and likely even include videos to help streamline the onramp for TA's to understand what is going on behind the covers of the automation.  Much automation in particular where CI could play an important role will be tackled later. Staff expectation is they will continue to grow and add additional capabilities in future classroom deployments.

We identified many DevOps practices that can be added to the project.  For this project, Phase I, we incorporated Infrastructure as Code, Automated Deployment, and Monitoring. Once we handed off the project to the customer, they immediately added single sign-on capabilities so students would not have to sign into Azure.  This was simple because we already had all the scripts to do the work using Bash so all that was needed was automatically lay down a certificate on each of the VMs so the VMs the students were using could auto-authenticate to other systems that were deployed and to Azure Active Directory. After this class, they plan on including automated testing and continuing to improve the processes we established in this project.  Full CI/CD is the next big challenge.   

## Project objectives ##


### Azure Command Line (Azure-Cli) using JSON Templates ###

The scripts assume you’ve logged into the Azure-CLI and selected the subscription you want to target. They provision a VM that is used as what we refer to as a gold image. This is the machine image that will be used for student machines. These images are then generalized and copied into a location that is accessible from other student subscriptions. 

A script the students will run is provided. This script pulls the gold image from the shared location into a new storage account in the student’s subscription. It then uses an ARM template that references the gold image to deploy the student VM. The default deployment is using Bash and Azure-CLI but there are other links that can be used for classes that are not Linux heavy.
All source code can be found on [github](http:/github.com/dstolts/Azure_Classroom)


![Azure-CLI](/images/2016-12-05-classroom/classroom20-azure-cli_screenshot.png)

### PowerShell ###
For the PowerShell Scripts, there are 4 main scripts to build the lab and execute the deployment.
- login.ps1
Logs the user into both Azure CLI and Azure PowerShell.![Using login.ps1](/images/2016-12-05-classroom/classroom30-ps-login_example.png)

- createbasevm.ps1
Uses Azure CLI to quickly create a Linux VM using a certificate instead of password authentication. The pub/private key pair is provided for convenience in the repo. This is not the same key The University used. It is just a sample to make trying it easy. We recommend generating your own keys using a tool like [Azure Key Vault] (https://azure.microsoft.com/en-us/services/key-vault/). [PuTTY]( http://www.putty.org/) is an open source program that can be used to generate keys. Upon successful completion, the SSH connection string and the deprovisioning command will also be pushed out to the console for the end user to use.  

![Using createbasevm.ps1](/images/2016-12-05-classroom/classroom31-ps-createbasevm_example1.png)
![Using createbasevm.ps1](/images/2016-12-05-classroom/classroom32-ps-createbasevm_example2.png)

-- captureimage.ps1
Uses positional parameters to capture the resource group name and VM name in plain text when executing the PS script. The script will then capture the VM created using the previous createbasevm.ps1 script and copy the image VHD to a public storage account. After the copy completes, an Image URI is printed to the output of the screen for the end user to use with the following script.

![Using captureimage.ps1](/images/2016-12-05-classroom/classroom33-ps-captureimage_example2.png)

![Using captureimage.ps1](/images/2016-12-05-classroom/classroom34-ps-captureimage_example1.png)

- deployVM.ps1
Uses positional parameters to capture a NEW resource group name and Image URI from the 3rd script in plain text when executing the PS script. The script will then copy the VHD from the public storage account to the user's local storage account in their subscription. From there, the script will use the image to complete the deployment using the associated JSON template files, which can be found in the templates folder.

![Using deployVM.ps1](/images/2016-12-05-classroom/classroom37-ps-deployVM_example1.png)

![Using deployVM.ps1](/images/2016-12-05-classroom/classroom38-ps-deployVM_example2.png)

- deployVM.ps1 creates a Custom Storage Account Parameters file as seen below:

![Using CustStorageAcct.parameters.json](/images/2016-12-05-classroom/classroom36-ps-CustStorageAcct.parameters_example.png)

- deployVM.ps1 also creates a Custom Gold VM Parameters file as seen below:

![Using CustomGoldVM.Parameters.json](/images/2016-12-05-classroom/classroom35-ps-CustGoldVM.parameters_example.png)


## Automated Deployment, Testing, Monitoring, Automated Destruction, CI/CD ##


## General lessons ##
Some key learnings to consider from this process:
 
## Conclusion ##

## Resources ##
  - Source Code on [github](http:/github.com/dstolts/shipping) 